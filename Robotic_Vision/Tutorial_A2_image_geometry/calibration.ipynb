{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "      <td><div align=\"left\"><font size=\"20\" >Camera Calibration</font></div></td>\n",
    "     <td><img src=\"images/RVSS-logo.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to invesitage how we can calibrate a camera to determine the camera's intrinsic and extrinsic parameters. When we calibrate a camera we take an image of a calibration rig and associate certain points/features within that image to real-world coordinates. We can then determine the camera parameters using an optimisation method, for example least-squares.  \n",
    "\n",
    "Before we dive into real-world data we are going to create a situation where we know the camera parameters, the real-world points and where these points would lie if an image was taken from a specific pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install machinevision-toolbox-python spatialmath-python # install MVTB and spatialmath\n",
    "\n",
    "import matplotlib\n",
    "import ipywidgets as wdg\n",
    "\n",
    "from machinevisiontoolbox import Image, CentralCamera\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from spatialmath import SE3, SO3\n",
    "from spatialmath import base\n",
    "\n",
    "from camera_calib_helpers import CreateSimulatedCamera\n",
    "\n",
    "np.set_printoptions(linewidth=120, formatter={'float': lambda x: f\"{x:8.4g}\" if abs(x) > 1e-10 else f\"{0:8.4g}\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration Rig\n",
    "\n",
    "RVSS2021 will be using the following calibration rig which consists of 12 dots forming three 6cm squares on three planes. We will store the real-world 3D coordinates that reflect the markers on the calibration target in the matrix `P_calib`. We will be using `P_calib` throughout this coding session.\n",
    "\n",
    "![calibration](images/calibration-fixture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 0.01 # centimetre to metre conversion factor\n",
    "\n",
    "P_calib = np.array([\n",
    "    [ 0,  -12.2, 12.2],\n",
    "    [ 0,   -6.2, 12.2],\n",
    "    [ 0,  -12.2,  6.2],\n",
    "    [ 0,   -6.2,  6.2],\n",
    "    [ 6.2,  0,   12.2],\n",
    "    [12.2,  0,   12.2],\n",
    "    [ 6.2,  0,    6.2],\n",
    "    [12.2,  0,    6.2]\n",
    "]).T * cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrating A Simulated Camera\n",
    "\n",
    "## Simulate a Camera\n",
    "\n",
    "First, we will simulate a black box camera. While we know the pose and chateristics of this camera (you can check them out in the `CreateSimulatedCamera` function, but we will assume we do not and see if we can find the camera extrinsic and intrinsic parameters ourself using simulated calibration data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_camera = CreateSimulatedCamera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `unknown_camera` is a model of the camera and was computed using the `CreateCameraModel` function in the `camera_calib_helpers.py` script. For now we will pretend we don't know it's characteristics, and that its pose is unknown with respect to the calibration target.\n",
    "\n",
    "However **we do know** the world coordinates of the markers.\n",
    "\n",
    "We can create an \"artificial\" image of the 3D points by projecting these points onto the 2D image plane within our simulated camera, by using the code below. *We can only do this because we know camera characteristics, as well as the pose of the camera and the locations of the 3D points with respect to the world frame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_camera.plot(P_calib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projecting the 3D coordinates of the calibration target markers onto the  into 2D image plane coordinates using the camera object's `project` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = unknown_camera.project(P_calib)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, with real camera calibration these 2D coordinates must be obtained by taking an image and finding the coordinates of the points in the captured image. We will be using a real-image later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Camera Matrix \n",
    "\n",
    "We can noe compute the camera calibration from the calibration data, the real-world coordinates `P_calib` and the 2D image coordinates `p`.\n",
    "\n",
    " * `P_calib`, is an array of 3D points which is a $3 \\times N$ matrix, one point per column (which would be found by measuring the points in the real-world)\n",
    " * `p`, is an array of 2D points which is a $2 \\times N$ matrix, one point per column (which would be found in the captured image)\n",
    "    \n",
    "Importantly, the points correspond, that is the $i^{th}$ column of `p` is the projection of the 3D point in the $i^{th}$ column of `P_calib`.\n",
    "\n",
    "The camera matrix $C$ can be computed by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = CentralCamera.camcal(P_calib, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the small residual is an encouraging sign, it indicates that the data is a good fit to the projection model of a perspective camera. *This is to expected in this case as we found `p` by projecting the 3D points using our artificial camera model*\n",
    "\n",
    "The value of the camera matrix is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intrinsic and extrinsic parameters of the camera are jumbled up in these 12 numbers, but we can unjumble them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CentralCamera.invcamcal(C)\n",
    "print(camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intrinsic and extrinsic parameters have been encapsulated in a toolbox `CentralCamera` object.\n",
    "\n",
    "**Note:** When computing the camera matrix the focal length and the pixel dimensions are multiplied together, so it's not possible to determine them individually.  This function assumes that the pixel dimension is 1 and the focal length has units of pixels rather than metres (hence the large values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intrinsic parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values in the top left are half image width and height we specified and that focal length (1500) is a factor of the length we provided into the `CreateSimulatedCamera` function. Naturally, because we are passing in exact data then getting back exact values is to be expected.\n",
    "\n",
    "The extrinsic parameters, the pose of the camera, with respect to the world calibration frame is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tcam = camera.pose\n",
    "print(Tcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates the position of the camera is (1, -1) on the ground plane, and slightly above the ground (z=0.1). Again, this is unsurprising as this is exactly what we passed into the `CreateCameraModel` function.\n",
    "\n",
    "The orientation of the camera can be found by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tcam.rpy(order='zyx', unit='deg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which indicates the orientation of the camera obtained is, in order, by:\n",
    "\n",
    "1. a rotation of 50° about the z-axis, which points the camera toward the calibration target\n",
    "2. a rotation of 2° about the y-axis, which indicates some imperfection, a small twist about the camera axis\n",
    "3. a rotation of -92° about the x-axis, which makes the camera's z-axis parallel to the ground plane, again some imperfection so the camera is actually pointing slightly toward the floor.\n",
    "\n",
    "Again, these values are what we specified in the `CreateCameraModel` and hence what we expect.\n",
    "\n",
    "That is all that is required to determine the camera charateristics. You simply need two sets of points, one from the real-world and one with the corresponding 2D image coordinates, you need at least 6 corresponding points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Ground Plane Coordinates to the Image Plane\n",
    "\n",
    "The extrinsic parameters indicate where the camera is with respect to the world frame. Well in many scenarios we want to know the coordinate of a point/object relative to our camera along the ground plane. We define the frame of this ground plane to be directly underneath the camera with the x-axis being forward and the z-axis being upward. We also want to take into account the imperfections in the camera orientation and its height about the ground plane.\n",
    "\n",
    "The x,y,z position of the camera relative to this ground-plane frame directly below the camera is therefore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0, 0, Tcam.t[2]] # Tcam.t[2] is the position of the camera in the world frame z-axis\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x and y components should always be 0 in fact. The orientation of the camera relative to the ground plane will only consist of the roll and pitch values we originally set. The yaw component will always be zero. Therefore the transform of the camera relative to this ground plane frame, $T$, will be given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpy = Tcam.rpy(order='zyx', unit='deg')\n",
    "rpy[2] = 0  # set the yaw component to zero\n",
    "T = SE3(t) * SE3.RPY(rpy, order='zyx', unit='deg')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the camera pose to be this transform, then we can determine how points in this ground plane frame would be projected into the image plane. This means we could do the inverse to determine the position of 2D image points in this ground plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.pose = T\n",
    "camera.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points on the ground 0.5m and 0.6m in front of the camera appear at the following locations on the image plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ptns = np.array([[0, 0.5, 0], [0, 0.6, 0]]).T\n",
    "camera.project(world_ptns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are close to u-coordinate of the principal point (640), indicating it close to the centre line.  Remember that the camera is slightly skewed with respect to the world frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping image plane coordinates to the ground plane\n",
    "\n",
    "In the previous section we intuitively worked through how we can map ground points into the image plane. Now we will see how can we do the opposite, but we will take a more principled approach. \n",
    "\n",
    "Points on the ground plane have a z-coordinate of zero.  The camera projection equation, in matrix form, is\n",
    "$$\n",
    "\\tilde{p} = \\begin{pmatrix} c_1 & c_2 & c_3  & c_4 \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    "where $c_i$ is the $i^{th}$ column of the camera matrix.  Since $Z=0$ we can rewrite as \n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{p} & = \\begin{pmatrix} c_1 & c_2 & c_4 \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\\\ 1 \\end{pmatrix} \\\\\n",
    "          & = \\mathbf{H} \\begin{pmatrix} X \\\\ Y \\\\ 1 \\end{pmatrix}\n",
    "\\end{align}\n",
    "where $(X, Y)$ is a point on the ground plane with respect to the camera frame which has the Y-axis forward and the X-axis to the right.\n",
    "\n",
    "In this case $\\mathbf{H}$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = camera.C[:,[0,1,3]]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is both square and non-singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means we have an invertible mapping between 2D points in the image plane and 2D points on the ground plane, both expressed in homogenous form.  Such a transformation is called an *homography*.\n",
    "\n",
    "**Note that in general we cannot map a 2D point to a 3D point, since a 2D image plane coordinate corresponds to a ray in space -- an infinite number of points.  However the constraint that the point lies on the ground plane makes a unique solution possible, it is the point where that ray intersects the ground plane.**\n",
    "\n",
    "Consider a point in the image plane at $(300, 600)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (300, 600)  # make homogeneous\n",
    "P = base.h2e(np.linalg.inv(H) @ base.e2h(p))\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if we project that using our camera model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = np.r_[P, 0]\n",
    "\n",
    "# appears to be bug in MVTB toolbox cannot pass in a (3,) or (3,1) array for camera.project\n",
    "# so we will simply create a copy of the point to make it a (3x2)\n",
    "pts = np.repeat(point.reshape((3,1)), 2, axis=1) \n",
    "camera.project(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we obtain, as we should do, the image plane coordinate we started with.\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. Using corresponding 3D and 2D data we can estimate a camera matrix.\n",
    "2. We can untangle the elements of that matrix into intrinsic and extrinsic parameters.\n",
    "3. We use an estimated camera model to form an homography which maps between image plane and ground plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you get to apply what you've learned above using a real image of a calibration target. The next section of code will display an image and a text widget showing an $8 \\times 2$ array. \n",
    "\n",
    "You will need to:\n",
    "\n",
    "1. click the centre of each point as accurately as you can, in the order given by the diagram at the top of this notebook.\n",
    "2. If you make a mistake, do a right-click and will undo the last point you clicked (and remove the marker)\n",
    "\n",
    "Once you are happy with the points you selected, move onto the next set of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "image = Image('images/calib-image.png', grey=True)\n",
    "fig = matplotlib.pyplot.figure()\n",
    "plt.imshow(image.image, cmap='gray')\n",
    "\n",
    "# Variables, p will contains clicked points, idx contains current point that is being selected\n",
    "p = np.ones((8,2)) * -1\n",
    "idx = 0\n",
    "\n",
    "# Create text area widget to display clicked locations\n",
    "txt = wdg.Textarea(value=str(p), placeholder='', description='Clicked Point:', disabled=False)\n",
    "display(txt)\n",
    "\n",
    "# Code to pick points and display in text widget\n",
    "def onclick(event):\n",
    "    global p, idx\n",
    "    \n",
    "#     txt.value = str(event)\n",
    "    if event.button == 1:\n",
    "        # left mouse click, add point and increment by 1\n",
    "        p[idx, 0] = event.xdata\n",
    "        p[idx, 1] = event.ydata\n",
    "        idx = idx + 1\n",
    "    elif event.button == 3:\n",
    "        # right click, go back to previous point\n",
    "        idx -= 1\n",
    "        p[idx, 0] = -1\n",
    "        p[idx, 1] = -1\n",
    "        \n",
    "    txt.value = str(p) \n",
    "    idx = np.min(np.max(idx, 0), 7) # to keep within bounds\n",
    "    \n",
    "ka = fig.canvas.mpl_connect('button_press_event', onclick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.T # we actually need a 2 x 8 matrix for the function below, so simply take the transpose\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now estimate the camera matrix, and extract the intrinsic and extrinsic parameters.  Compute the homography and see if you can sensibly map points between image plane and ground plane.\n",
    "\n",
    "**Once you have the camera intrinsic matrix K, revist the Aruco marker exercise from A1 using the proper value of K, and see if the estimates of marker pose agree with what you can measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
